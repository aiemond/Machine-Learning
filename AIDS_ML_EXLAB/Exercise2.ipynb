{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef6700b",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "### In this Exercise, you are given a dataset with K=200 unique categories, so each datapoint is a category between 1 and 200. Your task is to calculate the parameters, theta, of the underlying multinomial distribution, using MLE and MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e8e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 200\n",
      "seen categories K': 164\n",
      "N_train: 400, N_val: 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "data = df['category'].values\n",
    "K = 200 \n",
    "\n",
    "# usually this is not done in practice since this kind of split is biased by order of data! \n",
    "split_at = int(len(data) * 0.8) \n",
    "train = data[:split_at]\n",
    "val = data[split_at:]\n",
    "\n",
    "print(f'K: {K}')\n",
    "print(f'seen categories K\\': {np.unique(train).size}')\n",
    "print(f'N_train: {len(train)}, N_val: {len(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d7eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def counts_from_data(data, K):\n",
    "    \"\"\"\n",
    "    Count occurrences per category, assuming categories are 1..K.\n",
    "    Returns a length-K array of counts in order 1..K.\n",
    "    \"\"\"\n",
    "    # bincount with minlength K+1 to include index 0; drop index 0\n",
    "    counts = np.bincount(data, minlength=K+1)[1:]\n",
    "    if counts.shape[0] != K:\n",
    "        raise ValueError(\"Counts length mismatch with K. Check category labels and K.\")\n",
    "    return counts.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c437df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(data, K):\n",
    "    \"\"\"\n",
    "    MLE for multinomial parameters theta over K categories (1..K).\n",
    "    theta_k = n_k / N\n",
    "    \"\"\"\n",
    "    counts = counts_from_data(data, K)\n",
    "    N = counts.sum()\n",
    "    if N == 0:\n",
    "        raise ValueError(\"Empty data for MLE.\")\n",
    "    theta_mle = counts / N\n",
    "    return theta_mle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec91d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of category probabilities equal to zero (MLE): 37 of 200\n"
     ]
    }
   ],
   "source": [
    "theta_mle = mle(train, K)\n",
    "assert np.isclose(np.sum(theta_mle), 1.0)\n",
    "print(f\"Amount of category probabilities equal to zero (MLE): {(theta_mle==0).sum()} of {K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a430938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_estimate(data, K, alpha):\n",
    "    \"\"\"\n",
    "    MAP estimate for multinomial with symmetric Dirichlet(alpha) prior.\n",
    "    Mode of Dirichlet posterior: (n_k + alpha - 1) / (N + K*(alpha - 1)),\n",
    "    but when alpha <= 1 and n_k == 0, the mode is on boundary at 0.\n",
    "    here implement this by clipping negative numerators to 0 and renormalizing.\n",
    "    \"\"\"\n",
    "    if alpha is None:\n",
    "        raise ValueError(\"alpha must be provided for MAP.\")\n",
    "    counts = counts_from_data(data, K)\n",
    "    N = counts.sum()\n",
    "\n",
    "    # Numerator for MAP mode\n",
    "    numerators = counts + (alpha - 1.0)\n",
    "\n",
    "    # If alpha <= 1 and some counts are zero, the MAP mode is on boundary (0) for those k.\n",
    "    # Clipping to 0 implements the boundary solution correctly.\n",
    "    numerators = np.maximum(numerators, 0.0)\n",
    "\n",
    "    # Sum of numerators:\n",
    "    s = numerators.sum()\n",
    "    if s == 0:\n",
    "        # This would happen only in degenerate cases; fall back to uniform\n",
    "        theta_map = np.full(K, 1.0 / K, dtype=np.float64)\n",
    "    else:\n",
    "        theta_map = numerators / s\n",
    "\n",
    "    # For alpha > 1 and no clipping, theta_map matches the closed form denominator (N + K*(alpha-1))\n",
    "    # because sum(n_k + alpha - 1) = N + K*(alpha - 1).\n",
    "    return theta_map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c797bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of category probabilities equal to zero (MAP, alpha=1.1): 0 of 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose a value for alpha: (using >1 to avoid zeros; alpha=1 equals MLE)\n",
    "# A mild smoothing often works well, e.g., 1.1 or 1.01.\n",
    "\n",
    "ALPHA = 1.1\n",
    "theta_map = map_estimate(train, K, alpha=ALPHA)\n",
    "assert np.isclose(np.sum(theta_map), 1.0)\n",
    "print(f\"Amount of category probabilities equal to zero (MAP, alpha={ALPHA}): {(theta_map==0).sum()} of {K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72698e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data, theta):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood of data under multinomial parameters theta.\n",
    "    Using counts: sum_k n_k * log(theta_k).\n",
    "    Adds small epsilon to avoid log(0).\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    # Map categories (1..K) to indices (0..K-1)\n",
    "    K = theta.shape[0]\n",
    "    counts = counts_from_data(data, K)\n",
    "    ll = float(np.sum(counts * np.log(theta + eps)))\n",
    "    return ll\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ed5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "ll_mle_train = log_likelihood(train, theta_mle)\n",
    "ll_mle_val = log_likelihood(val, theta_mle)\n",
    "\n",
    "ALPHA_MLE = 1  # MAP with alpha=1 should equal MLE (see explanation below)\n",
    "theta_map_alpha1_val = map_estimate(train, K, alpha=ALPHA_MLE)\n",
    "ll_map_alpha1_val = log_likelihood(val, theta_map_alpha1_val)\n",
    "\n",
    "ll_map = log_likelihood(val, theta_map)\n",
    "ll_map_train = log_likelihood(train, theta_map)\n",
    "ll_map_val = log_likelihood(val, theta_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055f612",
   "metadata": {},
   "source": [
    "ALPHA=1 on the validation set should give the same log likelihood as the MLE (can you explain why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba74fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Log-Likelihood MLE: -1968.25\n",
      "[TRAIN] Log-Likelihood MAP (alpha=1.1): -1971.90\n",
      "[VAL]   Log-Likelihood MLE: -810.67\n",
      "[VAL]   Log-Likelihood MAP with alpha=1: -810.67\n",
      "[VAL]   Log-Likelihood MAP with alpha=1.1: -559.68\n"
     ]
    }
   ],
   "source": [
    "print(f\"[TRAIN] Log-Likelihood MLE: {ll_mle_train:.2f}\")\n",
    "print(f\"[TRAIN] Log-Likelihood MAP (alpha={ALPHA}): {ll_map_train:.2f}\")\n",
    "print(f\"[VAL]   Log-Likelihood MLE: {ll_mle_val:.2f}\")\n",
    "print(f\"[VAL]   Log-Likelihood MAP with alpha=1: {ll_map_alpha1_val:.2f}\")\n",
    "print(f\"[VAL]   Log-Likelihood MAP with alpha={ALPHA}: {ll_map_val:.2f}\")                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa0c23",
   "metadata": {},
   "source": [
    "Choosing α (ALPHA):\n",
    "\n",
    "α = 1: no smoothing; equal to MLE; zero probabilities for unseen categories. It reduces to the MLE, which is purely based on observed frequencies.Therefore, the probability estimates and the resulting log-likelihoods are identical.\n",
    "α > 1: additive smoothing; reduces overfitting to the training set; often improves validation likelihood/robustness.\n",
    "α ∈ [1.01,2] is a reasonable range to try first. Using the provided alpha sweep to pick what maximizes validation log-likelihood.\n",
    "MAP with α=1 does not add any smoothing or prior information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c4a8e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
